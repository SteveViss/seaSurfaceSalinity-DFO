---
title: "Perform Random forest for interpolation"
---

## Sample training data

Uniform sampling along OC distribution for each day
TB could be NA
OC values is in the overlap area of TB
TB and OC overlap in space and time

```{r}
library(feather)
files <- list.files("data/Feather", pattern = "*.feather", full.names = TRUE)
files <- data.frame(
    path = files,
    timestamp = as.Date(stringr::str_extract(files, "[0-9]{4}-[0-9]{2}-[0-9]{2}"))
)

nGroup <- 50
nSampleByGroup <- 10

### Prepare cluster
future::plan(future::multicore, workers = 10)

training_data <- files |>
    dplyr::filter(lubridate::year(timestamp) >= 2015) |>
    dplyr::pull(path) |>
    furrr::future_map_dfr(\(x) {
        d_sf <- feather::read_feather(x) |>
            dplyr::filter(!is.na(OC) & !is.na(TB)) |>
            sf::st_as_sf(coords = c("lon", "lat"), crs = 4326) |>
            sf::st_filter(sf::read_sf("data/interpolation_area.shp"))

        if (nrow(d_sf) > (nGroup * nSampleByGroup)) {
            as.data.frame(d_sf) |>
                dplyr::bind_cols(sf::st_coordinates(d_sf)) |>
                dplyr::select(-geometry) |>
                dplyr::rename(lat = Y, lon = X) |>
                dplyr::mutate(sampling_group = as.numeric(ggplot2::cut_number(OC, nGroup))) |>
                dplyr::group_by(sampling_group) |>
                dplyr::slice_sample(n = nSampleByGroup) |>
                dplyr::ungroup() |>
                dplyr::select(-sampling_group)
        }
    }, .progress = TRUE)

saveRDS(training_data, "data/randomForest/50bins_10samples/training_data_daily.rds")
```

### Overall locations

```{r}
training_set_locations <- dplyr::select(training_data, lon, lat) |>
    dplyr::distinct() |>
    sf::st_as_sf(coords = c("lon", "lat"), crs = 4326) |>
    sf::st_coordinates()

### Create density map - spatial coverage
```

### Validate sampled variable distribution 

1. Does is it differs from original dataset? Is the sampling representative of the initial distribution of the predictors.
2. Does values temporally cover? (count by julian day)

## Random forest

### Train base model

```{r}
training_data <- readRDS("data/randomForest/50bins_10samples/training_data_daily.rds") |>
    dplyr::mutate(
        julianDay = lubridate::yday(date),
        year = lubridate::year(date)
    ) |>
    dplyr::select(-is_origin, -IS, -date)

# number of features
n_features <- length(setdiff(names(training_data), "OC"))

# train a default random forest model
default_rf1 <- ranger::ranger(
    OC ~ .,
    data = training_data,
    mtry = floor(n_features / 3),
    respect.unordered.factors = "order",
    seed = 123
)

# get OOB RMSE
(default_rmse <- sqrt(default_rf1$prediction.error))

saveRDS(default_rf1, "data/randomForest/50bins_10samples/default_ref_rf1.rds")
```

#### Hyperparameters tunning 

https://bradleyboehmke.github.io/HOML/random-forest.html
https://statmath.wu.ac.at/~hornik/DTM/Presentations/pres_ranger.pdf

```{r}
hyper_grid <- expand.grid(
    mtry = 1:n_features,
    min.node.size = c(1, 3, 5, 10),
    replace = FALSE,
    sample.fraction = c(.5, .63, .8),
    rmse = NA
)

# execute full cartesian grid search
for (i in seq_len(nrow(hyper_grid))) {
    # fit model for ith hyperparameter combination
    fit <- ranger::ranger(
        formula = OC ~ .,
        data = training_data,
        num.trees = n_features * 10,
        mtry = hyper_grid$mtry[i],
        min.node.size = hyper_grid$min.node.size[i],
        replace = hyper_grid$replace[i],
        sample.fraction = hyper_grid$sample.fraction[i],
        verbose = FALSE,
        seed = 123,
        respect.unordered.factors = "order",
    )
    # export OOB error
    hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
}

hyper_grid <- hyper_grid |>
    dplyr::arrange(rmse) |>
    dplyr::mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100)

saveRDS(hyper_grid, "data/randomForest/50bins_10samples/hypergrid.rds")
```

RMSE is computed with sub-sample within the training set the for hyperparameters tuning

### Model validation

#### RMSE vs insitu data

```{r}
library(feather)
files <- list.files("data/Feather", pattern = "*.feather", full.names = TRUE)
files <- data.frame(
    path = files,
    timestamp = as.Date(stringr::str_extract(files, "[0-9]{4}-[0-9]{2}-[0-9]{2}"))
)

### Prepare cluster
future::plan(future::multicore, workers = 10)

insitu_set <- files |>
    dplyr::filter(lubridate::year(timestamp) >= 2015) |>
    dplyr::pull(path) |>
    furrr::future_map_dfr(\(x) {
        d_sf <- feather::read_feather(x) |>
            dplyr::filter(!is.na(IS) & !is.na(TB)) |>
            sf::st_as_sf(coords = c("lon", "lat"), crs = 4326) |>
            sf::st_filter(sf::read_sf("data/interpolation_area.shp"))
        if (nrow(d_sf) > 1) {
            as.data.frame(d_sf) |>
                dplyr::bind_cols(sf::st_coordinates(d_sf)) |>
                dplyr::select(-geometry) |>
                dplyr::rename(lat = Y, lon = X)
        }
    }, .progress = TRUE)
```

#### Get importance of variable in model prediction


> The basic idea of the permutation variable importance approach [18] is to consider a variable important if it has a positive effect on the prediction performance. To evaluate this, a tree is grown in the first step, and the prediction accuracy in the OOB observations is calculated. In the second step, any association between the variable of interest x_i and the outcome is broken by permuting the values of all individuals for x_i, and the prediction accuracy is computed again. The difference between the two accuracy values is the permutation importance for x_i from a single tree. The average of all tree importance values in a random forest then gives the random forest permutation importance of this variable. The procedure is repeated for all variables of interest. The package ranger [24] was used in our analyses.

From: https://github.com/imbs-hl/ranger/issues/237
See: https://academic.oup.com/bioinformatics/article/34/21/3711/4994791

```{r}
hyper_grid <- readRDS("data/randomForest/50bins_10samples/hypergrid.rds")

training_data <- readRDS("data/randomForest/50bins_10samples/training_data_daily.rds") |>
    dplyr::mutate(
        julianDay = lubridate::yday(date),
        year = lubridate::year(date)
    ) |>
    dplyr::select(-is_origin, -IS, -date)

# Hyperparameters selected
rf_impurity <- ranger::ranger(
    formula = OC ~ .,
    data = training_data,
    num.trees = 100,
    mtry = hyper_grid[1, "mtry"],
    min.node.size = hyper_grid[1, "min.node.size"],
    replace = hyper_grid[1, "replace"],
    sample.fraction = hyper_grid[1, "sample.fraction"],
    respect.unordered.factors = "order",
    importance = "impurity",
    verbose = FALSE,
    seed = 123
)

saveRDS(rf_impurity, "data/randomForest/50bins_10samples/rf_impurity.rds")

# re-run model with permutation-based variable importance
rf_permutation <- ranger::ranger(
    formula = OC ~ .,
    data = training_data,
    num.trees = 100,
    mtry = hyper_grid[1, "mtry"],
    min.node.size = hyper_grid[1, "min.node.size"],
    replace = hyper_grid[1, "replace"],
    sample.fraction = hyper_grid[1, "sample.fraction"],
    importance = "permutation",
    respect.unordered.factors = "order",
    verbose = FALSE,
    seed = 123
)

saveRDS(rf_permutation, "data/randomForest/50bins_10samples/rf_permutation.rds")

library(ggplot2)
p1 <- vip::vip(rf_impurity, num_features = n_features, bar = FALSE) + 
    ggtitle("Impurity")
p2 <- vip::vip(rf_permutation, num_features = n_features, bar = FALSE) + 
    ggtitle("Permutation")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

#### Compute figure

### Validate model

1. Figure predicted vs observed with Psquared and RMSE 
2. Spatial prediction, uniform sampling along NA maps and interpolate to see the result  

### Notes

There are four main practical disadvantages of RF:

- Depending on data and assumptions about data, it can over-fit values without an analyst even noticing it.
- It predicts well only within the feature space with enough training data. Extrapolation i.e. prediction outside the training space can lead to poor performance (Meyer & Pebesma, 2021).
- It can be computationally expensive with computational load increasing exponentially with the number of covariates.
- It requires quality training data and is highly sensitive to blunders and typos in the data.

Source - https://opengeohub.github.io/spatial-prediction-eml/introduction-to-spatial-and-spatiotemporal-data.html

## Questions Julien

1. Échelle de valeur pour le temps relatif au solstice. Revoir avec julien la fonction, ca semble pas trop clair, dans la fonction du package TIDML, ca semble être la position relative au solstice et non une fonction cos-sin. 
2. Problème de couverture temporelle pour la validation insitu. Note ca devrait plus être un problème pour si la couverture de TB augmente.
3. Problème si TB = NA, comment on impute le NA. voir le package randomForest qui le fait nativement mais pas dans le package ranger (voir https://cran.r-project.org/web/packages/missRanger/vignettes/missRanger.html).
